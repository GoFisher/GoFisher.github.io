<!-- build time:Mon Dec 06 2021 21:43:22 GMT+0800 (中国标准时间) --><!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#FFF"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><link rel="icon" type="image/ico" sizes="32x32" href="/images/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="alternate" type="application/rss+xml" title="余心所善，九死未悔！" href="http://example.com/rss.xml"><link rel="alternate" type="application/atom+xml" title="余心所善，九死未悔！" href="http://example.com/atom.xml"><link rel="alternate" type="application/json" title="余心所善，九死未悔！" href="http://example.com/feed.json"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CFredericka%20the%20Great:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20JP:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CInconsolata:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/css/app.css?v=0.2.5"><meta name="keywords" content="大数据技术与应用,Hive"><link rel="canonical" href="http://example.com/2021/12/02/%E5%9C%A8%E7%BA%BF%E8%AF%BE%E7%A8%8B/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8/Hive%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%93%8D%E4%BD%9C/"><title>Hive数据类型操作 - Hive安装教程 - 大数据技术与应用 - 在线课程 | Yu's Sky = 余心所善，九死未悔！</title><meta name="generator" content="Hexo 5.2.0"></head><body itemscope itemtype="http://schema.org/WebPage"><div id="loading"><div class="cat"><div class="body"></div><div class="head"><div class="face"></div></div><div class="foot"><div class="tummy-end"></div><div class="bottom"></div><div class="legs left"></div><div class="legs right"></div></div><div class="paw"><div class="hands left"></div><div class="hands right"></div></div></div></div><div id="container"><header id="header" itemscope itemtype="http://schema.org/WPHeader"><div class="inner"><div id="brand"><div class="pjax"><h1 itemprop="name headline">Hive数据类型操作</h1><div class="meta"><span class="item" title="创建时间：2021-12-02 00:49:21"><span class="icon"><i class="ic i-calendar"></i> </span><span class="text">发表于</span> <time itemprop="dateCreated datePublished" datetime="2021-12-02T00:49:21+08:00">2021-12-02</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span class="text">本文字数</span> <span>11k</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span class="text">阅读时长</span> <span>10 分钟</span></span></div></div></div><nav id="nav"><div class="inner"><div class="toggle"><div class="lines" aria-label="切换导航栏"><span class="line"></span> <span class="line"></span> <span class="line"></span></div></div><ul class="menu"><li class="item title"><a href="/" rel="start">Yu's Sky</a></li></ul><ul class="right"><li class="item theme"><i class="ic i-sun"></i></li><li class="item search"><i class="ic i-search"></i></li></ul></div></nav></div><div id="imgs" class="pjax"><ul><li class="item" data-background-image="https://tva1.sinaimg.cn/large/6833939bly1gipesx5fdwj20zk0m81kx.jpg"></li><li class="item" data-background-image="https://tva1.sinaimg.cn/large/6833939bly1gicitspjpbj20zk0m81ky.jpg"></li><li class="item" data-background-image="https://tva1.sinaimg.cn/large/6833939bly1gicliwyw55j20zk0m8hdt.jpg"></li><li class="item" data-background-image="https://tva1.sinaimg.cn/large/6833939bly1gipexoj0moj20zk0m8kgu.jpg"></li><li class="item" data-background-image="https://tva1.sinaimg.cn/large/6833939bly1giciszlczyj20zk0m816d.jpg"></li><li class="item" data-background-image="https://tva1.sinaimg.cn/large/6833939bly1gicit4jrvuj20zk0m8785.jpg"></li></ul></div></header><div id="waves"><svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"/></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"/><use xlink:href="#gentle-wave" x="48" y="3"/><use xlink:href="#gentle-wave" x="48" y="5"/><use xlink:href="#gentle-wave" x="48" y="7"/></g></svg></div><main><div class="inner"><div id="main" class="pjax"><div class="article wrap"><div class="breadcrumb" itemscope itemtype="https://schema.org/BreadcrumbList"><i class="ic i-home"></i> <span><a href="/">首页</a></span><i class="ic i-angle-right"></i> <span itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/%E5%9C%A8%E7%BA%BF%E8%AF%BE%E7%A8%8B/" itemprop="item" rel="index" title="分类于 在线课程"><span itemprop="name">在线课程</span></a><meta itemprop="position" content="1"></span><i class="ic i-angle-right"></i> <span itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/%E5%9C%A8%E7%BA%BF%E8%AF%BE%E7%A8%8B/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8/" itemprop="item" rel="index" title="分类于 大数据技术与应用"><span itemprop="name">大数据技术与应用</span></a><meta itemprop="position" content="2"></span><i class="ic i-angle-right"></i> <span class="current" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/%E5%9C%A8%E7%BA%BF%E8%AF%BE%E7%A8%8B/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8/Hive%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/" itemprop="item" rel="index" title="分类于 Hive安装教程"><span itemprop="name">Hive安装教程</span></a><meta itemprop="position" content="3"></span></div><article itemscope itemtype="http://schema.org/Article" class="post block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="http://example.com/2021/12/02/%E5%9C%A8%E7%BA%BF%E8%AF%BE%E7%A8%8B/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8/Hive%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%93%8D%E4%BD%9C/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="Hunter Yu"><meta itemprop="description" content=", "></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="余心所善，九死未悔！"></span><div class="body md" itemprop="articleBody"><h1 id="hive数据类型操作"><a class="anchor" href="#hive数据类型操作">#</a> Hive 数据类型操作</h1><h2 id="hive内部表操作"><a class="anchor" href="#hive内部表操作">#</a> Hive 内部表操作</h2><h3 id="针对基本类型建表"><a class="anchor" href="#针对基本类型建表">#</a> 针对基本类型建表</h3><p>首先，在 master 机器的 <code>/export/data</code> 目录下创建 hivedata 目录，在改文件夹下创建 <code>user.txt</code> ，并添加如下数据内容：</p><p></p><figure class="highlight txt"><figcaption><span>测试数据内容</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1,allen,18</span><br><span class="line">2,tom,23</span><br><span class="line">3,jerry,28</span><br></pre></td></tr></table></figure><p></p><p>具体操作如下：</p><p></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]<span class="comment"># mkdir -p /export/data/hivedata</span></span><br><span class="line">[root@master ~]<span class="comment"># cd /export/data/hivedata/</span></span><br><span class="line">[root@master hivedata]<span class="comment"># vi user.txt </span></span><br><span class="line">[root@master hivedata]<span class="comment"># cat user.txt </span></span><br><span class="line">1,allen,18</span><br><span class="line">2,tom,23</span><br><span class="line">3,jerry,28</span><br></pre></td></tr></table></figure><p></p><p>启动 hadoop 和 hive, 具体如下操作</p><p></p><figure class="highlight bash"><figcaption><span>bash代码</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@master data]<span class="comment"># sh /usr/local/hadoop-2.6.5/sbin/start-all.sh</span></span><br><span class="line">[root@master data]<span class="comment"># sh /export/servers/apache-hive-1.2.1-bin/bin/hive</span></span><br></pre></td></tr></table></figure><p></p><p><img data-src="https://gitee.com/vickyFish/typora_pic/raw/master/images/image-20211201232425173.png" alt="image-20211201232425173"></p><p>在 hive 界面输入 <code>set hive.cli.print.current.db=true</code> 设置显示当前使用数据库，然后创建 <code>zjdf</code> 数据库，创建数据库后可以使用 <code>show databases</code> 命令进行查看，如果已经存在，可以使用 <code>drop database database_name cascade</code> 命令强制性删除，如果有表格，一定要添加 <code>cascade</code> 参数，用 <code>use</code> 切换到这个新建的数据库，并创建内部表 <code>t_user</code> , 具体操作如下：</p><p></p><figure class="highlight plain"><figcaption><span>hive界面命令</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; set hive.cli.print.current.db&#x3D;true;</span><br><span class="line">hive (default)&gt; show databases;</span><br><span class="line">OK</span><br><span class="line">default</span><br><span class="line">Time taken: 0.023 seconds, Fetched: 1 row(s)</span><br><span class="line">hive (default)&gt; create database zjdf;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.182 seconds、  </span><br><span class="line">hive (default)&gt; show databases;</span><br><span class="line">OK</span><br><span class="line">default</span><br><span class="line">zjdf</span><br><span class="line">Time taken: 0.016 seconds, Fetched: 2 row(s)</span><br><span class="line">hive (default)&gt; use zjdf;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.024 seconds</span><br><span class="line">hive (zjdf)&gt;</span><br></pre></td></tr></table></figure><p></p><p><img data-src="https://gitee.com/vickyFish/typora_pic/raw/master/images/image-20211201233838557.png" alt="image-20211201233838557"></p><p>针对 hivedata 目录准备的结构化文件 user.txt 先创建一个内部表 t_user，具体示例如下：</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hive (zjdf)&gt; create table t_user(id int,name string,age int) ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;,&#39;;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.561 seconds</span><br><span class="line">hive (zjdf)&gt; show tables;</span><br><span class="line">OK</span><br><span class="line">t_user</span><br><span class="line">Time taken: 0.037 seconds, Fetched: 1 row(s)</span><br><span class="line">hive (zjdf)&gt; </span><br></pre></td></tr></table></figure><p></p><p>创建成功后，通过 WEB UI 界面打开 Hive 内部表所在 HDFS 路径（默认 /user/hive/warehouse/zidj.db/t_user）进行查看，如下图所示。</p><p><img data-src="https://gitee.com/vickyFish/typora_pic/raw/master/images/image-20211201234130736.png" alt="image-20211201234130736"></p><p>复制 master 终端，然后点击选项卡排列，可以达到左右分屏的效果，如下所示：</p><p><img data-src="https://gitee.com/vickyFish/typora_pic/raw/master/images/image-20211201234439467.png" alt="image-20211201234439467"></p><p>将准备好的 user.txt 移动到 hadoop 对应的 t_user 表的路径中，具体操作如下：</p><p></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@master hivedata]<span class="comment"># hadoop fs -put user.txt /user/hive/warehouse/zjdf.db/t_user</span></span><br><span class="line">21/12/01 10:05:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library <span class="keyword">for</span> your platform... using builtin-java classes <span class="built_in">where</span> applicable</span><br><span class="line">[root@master hivedata]<span class="comment">#</span></span><br></pre></td></tr></table></figure><p></p><p>刷新 Hadoop 文件系统，如下图所示：</p><p><img data-src="https://gitee.com/vickyFish/typora_pic/raw/master/images/image-20211201234942510.png" alt="image-20211201234942510"></p><p>在 hive 界面中查询 t_user 表数据，发现表中已经存在数据，具体命令如下：</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hive (zjdf)&gt; select * from t_user;</span><br><span class="line">OK</span><br><span class="line">1	allen	18</span><br><span class="line">2	tom	23</span><br><span class="line">3	jerry	28</span><br><span class="line">Time taken: 0.476 seconds, Fetched: 3 row(s)</span><br><span class="line">hive (zjdf)&gt;</span><br></pre></td></tr></table></figure><p></p><p><img data-src="https://gitee.com/vickyFish/typora_pic/raw/master/images/image-20211201235147810.png" alt="image-20211201235147810"></p><h3 id="针对复杂类型建表"><a class="anchor" href="#针对复杂类型建表">#</a> 针对复杂类型建表</h3><p>例如，现有结构化数据文件 student.txt，文件内容如下所示。</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1,zhangsan,唱歌:非常喜欢-跳舞:喜欢-游泳:一般般</span><br><span class="line">2,lisi,打游戏:非常喜欢-篮球:不喜欢</span><br></pre></td></tr></table></figure><p></p><p>通过对 student.txt 文件内容分析得出，可以设计为 3 列字段，即编号、姓名、兴趣，其中编号可以为 int 类型，姓名可以为 string 类型，而兴趣列还需要进一步分隔为 Map 类型，因此在创建 student.txt 文件对应的内部表语句如下所示：</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">hive (zjdf)&gt; create table t_student(id int,name string,hobby map&lt;string,string&gt;) row format delimited fields terminated by &#39;,&#39;  collection items terminated by &#39;-&#39; map keys terminated by &#39;:&#39;;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.064 seconds</span><br><span class="line">hive (zjdf)&gt; show tables;</span><br><span class="line">OK</span><br><span class="line">t_student</span><br><span class="line">t_user</span><br><span class="line">Time taken: 0.03 seconds, Fetched: 2 row(s)</span><br><span class="line">hive (zjdf)&gt;</span><br></pre></td></tr></table></figure><p></p><p>上述建表语句中，通过对 student.txt 文件结构化文件的分析，先通过逗号 “，” 对多个字段 fields 进行分隔；接着，针对 hobby 字段列，通过横线 “-” 进行集合列分隔；最后，再针对每一个爱好，通过冒号 “：” 进行分隔为最终的 “key：value” 形式。执行上述建表语句后，就会在默认的 /user/hive/warehouse/zjdf.db 文件夹下生成一个 t_student 文件夹。如下图所示：</p><p><img data-src="https://gitee.com/vickyFish/typora_pic/raw/master/images/image-20211201235638917.png" alt="image-20211201235638917"></p><p>此时，还必须将前面的结构化文件 student.txt 上传到该文件夹下进行映射，才能生成对应的内部表数据，上传完成后再次查询生成的 t_student 表信息。</p><p></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@master hivedata]<span class="comment"># vi student.txt</span></span><br><span class="line">[root@master hivedata]<span class="comment"># cat student.txt </span></span><br><span class="line">1,zhangsan,唱歌:非常喜欢-跳舞:喜欢-游泳:一般般</span><br><span class="line">2,lisi,打游戏:非常喜欢-篮球:不喜欢</span><br><span class="line">[root@master hivedata]<span class="comment"># hadoop fs -put student.txt /user/hive/warehouse/zjdf.db/t_student</span></span><br><span class="line">21/12/01 10:15:07 WARN util.NativeCodeLoader: Unable to load native-hadoop library <span class="keyword">for</span> your platform... using builtin-java classes <span class="built_in">where</span> applicable</span><br><span class="line">[root@master hivedata]<span class="comment">#</span></span><br></pre></td></tr></table></figure><p></p><p>再次在 hive 中查询 t_student 数据表格，如下图所示：</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive (zjdf)&gt; select * from t_student;</span><br><span class="line">OK</span><br><span class="line">1	zhangsan	&#123;&quot;唱歌&quot;:&quot;非常喜欢&quot;,&quot;跳舞&quot;:&quot;喜欢&quot;,&quot;游泳&quot;:&quot;一般般&quot;&#125;</span><br><span class="line">2	lisi	&#123;&quot;打游戏&quot;:&quot;非常喜欢&quot;,&quot;篮球&quot;:&quot;不喜欢&quot;&#125;</span><br><span class="line">Time taken: 0.082 seconds, Fetched: 2 row(s)</span><br><span class="line">hive (zjdf)&gt; </span><br></pre></td></tr></table></figure><p></p><h2 id="hive外部表操作"><a class="anchor" href="#hive外部表操作">#</a> Hive 外部表操作</h2><p>内部表，即不添加关键字 External，内部表与结构化数据文件要想产生关系映射，那么数据文件就必须在指定的内部表文件夹下，而当遇到大文件的情况时，移动数据文件非常耗时，这就需要我们来创建外部表，因为它不需要移动结构化数据文件。下面我们通过一个小案例来，对外部表进行讲解。</p><p>现有结构化数据文件 student.txt，且数据内容如文件所示。</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">95001,李勇,男,20,CS</span><br><span class="line">95002,刘晨,女,19,IS</span><br><span class="line">95003,王敏,女,22,MA</span><br><span class="line">95004,张立,男,19,IS</span><br><span class="line">95005,刘刚,男,18,MA</span><br><span class="line">95006,孙庆,男,23,CS</span><br><span class="line">95007,易思玲,女,19,MA</span><br><span class="line">95008,李娜,女,18,CS</span><br><span class="line">95009,梦圆圆,女,18,MA</span><br><span class="line">95010,孔小涛,男,19,CS</span><br><span class="line">95011,包小柏,男,18,MA</span><br><span class="line">95012,孙花,女,20,CS</span><br><span class="line">95013,冯伟,男,21,CS</span><br><span class="line">95014,王小丽,女,19,CS</span><br><span class="line">95015,王君,男,18,MA</span><br><span class="line">95016,钱国,男,21,MA</span><br><span class="line">95017,王风娟,女,18,IS</span><br><span class="line">95018,王一,女,19,IS</span><br><span class="line">95019,邢小丽,女,19,IS</span><br><span class="line">95020,赵钱,男,21,IS</span><br><span class="line">95021,周二,男,17,MA</span><br><span class="line">95022,郑明,男,20,MA</span><br></pre></td></tr></table></figure><p></p><p>首先，我们将 student.txt 文件上传至 HDFS 上的 /stu 路径下，用来模拟生产环境下的数据文件，具体命令如下所示：</p><p></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]<span class="comment"># vi student.txt </span></span><br><span class="line">[root@master ~]<span class="comment"># hadoop fs -mkdir /stu</span></span><br><span class="line">21/12/01 10:21:27 WARN util.NativeCodeLoader: Unable to load native-hadoop library <span class="keyword">for</span> your platform... using builtin-java classes <span class="built_in">where</span> applicable</span><br><span class="line">[root@master ~]<span class="comment"># hadoop fs -put student.txt /stu</span></span><br><span class="line">21/12/01 10:22:05 WARN util.NativeCodeLoader: Unable to load native-hadoop library <span class="keyword">for</span> your platform... using builtin-java classes <span class="built_in">where</span> applicable</span><br><span class="line">[root@master ~]<span class="comment">#</span></span><br></pre></td></tr></table></figure><p></p><p>在 hive 中创建外部表</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">hive (zjdf)&gt; create external table student_ext(Sno int,Sname string,Sex string,Sage int,Sdept string) row format delimited fields terminated by &#39;,&#39; location &#39;&#x2F;stu&#39;;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.041 seconds</span><br><span class="line">hive (zjdf)&gt; show tables;</span><br><span class="line">OK</span><br><span class="line">student_ext</span><br><span class="line">t_student</span><br><span class="line">t_user</span><br><span class="line">Time taken: 0.027 seconds, Fetched: 3 row(s)</span><br><span class="line">hive (zjdf)&gt; </span><br></pre></td></tr></table></figure><p></p><p>从结果发现，student_ext 外部表已经创建成功，最后，HQL（HiveQL）对数据表的内容的查看、增加、删除以及修改的语句均与 SQL 语句一致。下面以查看数据表内容为例进行演示，具体语法如下所示：</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (zjdf)&gt; select * from student_ext;</span><br></pre></td></tr></table></figure><p></p><p>结果如下</p><p><img data-src="https://gitee.com/vickyFish/typora_pic/raw/master/images/image-20211202001146733.png" alt="image-20211202001146733"></p><p><strong>小提示：Hive 创建内部表时，会将数据移动到数据库指向的路径；创建外部表时，仅记录数据所在的路径，不会对数据的位置做任何改变。在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据</strong></p><h2 id="hive分区表操作"><a class="anchor" href="#hive分区表操作">#</a> Hive 分区表操作</h2><p>分区表是按照属性在文件夹层面给文件更好的管理，实际上就是对应一个 HDFS 文件系统上的独立文件夹，该文件夹下是该分区所有的数据文件。Hive 中的分区就是分目录，把一个大的数据集根据业务需要分割成小的数据集。在查询时通过 WHERE 子句中的表达式选择查询指定的分区，这样的查询效率会提高很多。Hive 分区表一共有两种，分别为普通分区和动态分区，我们下面就要对其分别进行介绍。</p><h3 id="hive普通分区"><a class="anchor" href="#hive普通分区">#</a> Hive 普通分区</h3><p>创建分区表分为两种，一种是单分区，也就是说在表文件夹目录下只有一级文件夹目录。另外一种是多分区，表文件夹下出现多文件夹嵌套模式，现在我们只针对单分区进行详解，若想学习多分区可以参考官网的官方文档。</p><p>现有结构化数据文件 user_p.txt，文件中的数据内容如文件所示。</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1,allen</span><br><span class="line">2,tom</span><br><span class="line">3,jerry</span><br></pre></td></tr></table></figure><p></p><p></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]<span class="comment"># mkdir -p /export/data/hivedata/</span></span><br><span class="line">[root@master ~]<span class="comment"># cd /export/data/hivedata/</span></span><br><span class="line">[root@master hivedata]<span class="comment"># ll</span></span><br><span class="line">total 8</span><br><span class="line">-rw-r--r--. 1 root root 109 Dec  1 10:14 student.txt</span><br><span class="line">-rw-r--r--. 1 root root  31 Dec  1 09:43 user.txt</span><br><span class="line">[root@master hivedata]<span class="comment"># vi user_p.txt</span></span><br><span class="line">[root@master hivedata]<span class="comment">#</span></span><br></pre></td></tr></table></figure><p></p><p>首先，创建分区表。语法格式如下所示：</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (zjdf)&gt; create table t_user_p(id int, name string) partitioned by (country string) row format delimited fields terminated by &#39;,&#39;;</span><br></pre></td></tr></table></figure><p></p><p><img data-src="https://gitee.com/vickyFish/typora_pic/raw/master/images/image-20211202002144514.png" alt="image-20211202002144514"></p><p>其次，加载数据是将数据文件移动到与 Hive 表对应的位置，从本地（Linux）复制或移动到 HDFS 文件系统的操作。由于分区表在映射数据时不能使用 Hadoop 命令移动文件，需要使用 Load 命令，其语法格式如下所示：</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">LOAD DATA [LOCAL] INPATH &#39;filepath&#39; [OVERWRITE] </span><br><span class="line">INTO TABLE table_name [PARTITION (partcol1&#x3D;val1, partcol2&#x3D;val2 ...)]</span><br></pre></td></tr></table></figure><p></p><p>Load Data 是 HQL 固定的数据装载语句，下面针对部分关键字进行讲解。</p><ul><li>Filepath：它可以引用一个文件（在这种情况下，Hive 将文件移动到表所对应的目录中），或者它可以是一个目录（在这种情况下，Hive 将把该目录中的所有文件移动到表所对应的目录中）。它可以是相对路径、绝对路径以及完整的 URI。</li><li>Local：如果指定了 Local 键字，Load 命令将在本地文件系统（Hive 服务启动方）中查找文件路径，将其复制到对应的 HDFS 路径下；如果没有指定 Local 关键字，它将会从 HDFS 中移动数据文件至对应的表路径下。</li><li>Overwrite：如果使用了 Overwrite 关键字，当加载数据时目标表或分区中的内容会被删除，然后再将 filepath 指向的文件或目录中的内容添加到表或分区中。简单的说就是覆盖表中已有数据；若不添加该关键字，则表示追加数据内容。</li></ul><p>加载数据操作的语法格式如下所示：</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive (zjdf)&gt; load data local inpath &#39;&#x2F;export&#x2F;data&#x2F;hivedata&#x2F;user_p.txt&#39; into table t_user_p partition(country&#x3D;&#39;USA&#39;);</span><br><span class="line">Loading data to table zjdf.t_user_p partition (country&#x3D;USA)</span><br><span class="line">Partition zjdf.t_user_p&#123;country&#x3D;USA&#125; stats: [numFiles&#x3D;1, numRows&#x3D;0, totalSize&#x3D;22, rawDataSize&#x3D;0]</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.618 seconds</span><br><span class="line">hive (zjdf)&gt;</span><br></pre></td></tr></table></figure><p></p><p>从上述语句看出，Load Data 表示装载数据，Inpath 表示数据文件所在的本地系统路径，partition（country=‘USA’）为指定的分区，它需要与建表时设置的分区字段保持一致。执行完上述命令后，查看表内容的数据，效果如图所示：</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hive (zjdf)&gt; select * from t_user_p;</span><br><span class="line">OK</span><br><span class="line">1	allen	USA</span><br><span class="line">2	tom	USA</span><br><span class="line">3	jerry	USA</span><br><span class="line">Time taken: 0.274 seconds, Fetched: 3 row(s)</span><br><span class="line">hive (zjdf)&gt; </span><br></pre></td></tr></table></figure><p></p><h3 id="hive动态分区"><a class="anchor" href="#hive动态分区">#</a> Hive 动态分区</h3><p>上面介绍了 Hive 普通分区的创建和 Load 命令加载数据的操作。在默认情况下，我们加载数据时，需要手动的设置分区字段，并且针对一个分区就要写一个插入语句。如果源数据量很大时（例如，现有许多日志文件，要求按照日期作为分区字段，在插入数据的时候无法手动的添加分区），就可以利用 Hive 提供的动态分区，可以简化插入数据时的繁琐操作，若想实现动态分区，则需要开启动态分区功能，具体命令如下所示：</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; set hive.exec.dynamic.partition&#x3D;true;</span><br><span class="line">hive&gt; set hive.exec.dynamic.partition.mode&#x3D;nonstrict;</span><br></pre></td></tr></table></figure><p></p><p>Hive 默认是不支持动态分区的，因此 hive.exec.dynamic.partition 默认值为 false，需要启动动态分区功能，可以将该参数设置为 true；其中 hive.exec.dynamic.partition.mode 的默认值是 strict，表示必须指定至少一个分区为静态分区，将此参数修改为 nonstrict，表示允许所有的分区字段都可以使用动态分区。</p><p>在 Hive 中 insert 语句是用于动态插入数据的，不同的是它主要是结合 select 查询语句使用，且非常适用于动态分区插入数据，语法格式如下所示：</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; insert overwrite table table_name </span><br><span class="line">partition (partcol1[&#x3D;val1], partcol2[&#x3D;val2] ...) </span><br><span class="line">select_statement FROM from_statement</span><br></pre></td></tr></table></figure><p></p><p>现有原始表的结构化数据文件 dynamic_partition_table.txt，内容数据如文件所示。</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">2018-05-10,ip1</span><br><span class="line">2018-05-10,ip2</span><br><span class="line">2018-06-14,ip3</span><br><span class="line">2018-06-14,ip4</span><br><span class="line">2018-06-15,ip1</span><br><span class="line">2018-06-15,ip2</span><br></pre></td></tr></table></figure><p></p><p></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@master hivedata]<span class="comment"># vi dynamic_partition_table.txt</span></span><br><span class="line">[root@master hivedata]<span class="comment"># cat dynamic_partition_table.txt </span></span><br><span class="line">2018-05-10,ip1</span><br><span class="line">2018-05-10,ip2</span><br><span class="line">2018-06-14,ip3</span><br><span class="line">2018-06-14,ip4</span><br><span class="line">2018-06-15,ip1</span><br><span class="line">2018-06-15,ip2</span><br><span class="line">[root@master hivedata]<span class="comment">#</span></span><br></pre></td></tr></table></figure><p></p><p>现在我们通过一个案例进行演示动态分区的数据插入操作。将 dynamic_partition_table 中的数据按照时间（day），插入到目标表 d_p_t 的相应分区中。</p><p>首先，创建原始表。语法格式如下所示：</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">hive (zjdf)&gt; create table dynamic_partition_table(day string,ip string) row format delimited fields terminated by &quot;,&quot;;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.05 seconds</span><br><span class="line">hive (zjdf)&gt; show tables;</span><br><span class="line">OK</span><br><span class="line">dynamic_partition_table</span><br><span class="line">student_ext</span><br><span class="line">t_student</span><br><span class="line">t_user</span><br><span class="line">t_user_p</span><br><span class="line">Time taken: 0.033 seconds, Fetched: 5 row(s)</span><br></pre></td></tr></table></figure><p></p><p>其次，加载数据文件至原始表，语法格式如下所示：</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">hive (zjdf)&gt; load data local inpath &#39;&#x2F;export&#x2F;data&#x2F;hivedata&#x2F;dynamic_partition_table.txt&#39; into table dynamic_partition_table;</span><br><span class="line">Loading data to table zjdf.dynamic_partition_table</span><br><span class="line">Table zjdf.dynamic_partition_table stats: [numFiles&#x3D;1, totalSize&#x3D;90]</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.234 seconds</span><br><span class="line">hive (zjdf)&gt; select * from dynamic_partition_table;</span><br><span class="line">OK</span><br><span class="line">2018-05-10	ip1</span><br><span class="line">2018-05-10	ip2</span><br><span class="line">2018-06-14	ip3</span><br><span class="line">2018-06-14	ip4</span><br><span class="line">2018-06-15	ip1</span><br><span class="line">2018-06-15	ip2</span><br><span class="line">Time taken: 0.062 seconds, Fetched: 6 row(s)</span><br><span class="line">hive (zjdf)&gt; </span><br></pre></td></tr></table></figure><p></p><p>再次，创建目标表，语法格式如下所示：</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">hive (zjdf)&gt; create table d_p_t(ip string) partitioned by (month string,day string);</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.045 seconds</span><br><span class="line">hive (zjdf)&gt; show tables;</span><br><span class="line">OK</span><br><span class="line">d_p_t</span><br><span class="line">dynamic_partition_table</span><br><span class="line">student_ext</span><br><span class="line">t_student</span><br><span class="line">t_user</span><br><span class="line">t_user_p</span><br><span class="line">Time taken: 0.023 seconds, Fetched: 6 row(s)</span><br><span class="line">hive (zjdf)&gt;</span><br></pre></td></tr></table></figure><p></p><p>依次，动态插入，语法格式如下所示：</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (zjdf)&gt; insert overwrite table d_p_t partition (month,day) select ip,substr(day,1,7) as month,day from dynamic_partition_table;</span><br></pre></td></tr></table></figure><p></p><p><img data-src="C:/Users/yujz/AppData/Roaming/Typora/typora-user-images/image-20211202004106877.png" alt="image-20211202004106877"></p><p>最后，查看目标表中的分区数据，语法格式如下所示：</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hive (zjdf)&gt; show partitions d_p_t;</span><br><span class="line">OK</span><br><span class="line">month&#x3D;2018-05&#x2F;day&#x3D;2018-05-10</span><br><span class="line">month&#x3D;2018-06&#x2F;day&#x3D;2018-06-14</span><br><span class="line">month&#x3D;2018-06&#x2F;day&#x3D;2018-06-15</span><br><span class="line">Time taken: 0.449 seconds, Fetched: 3 row(s)</span><br><span class="line">hive (zjdf)&gt;</span><br></pre></td></tr></table></figure><p></p><h2 id="hive桶表操作"><a class="anchor" href="#hive桶表操作">#</a> Hive 桶表操作</h2><p>为了将表进行更细粒度的范围划分，我们可以创建桶表。桶表，是根据某个属性字段把数据分成几个桶（我们这里设置为 4，默认值是 - 1，可自定义），也就是在文件的层面上把数据分开。下面通过一个案例进行桶表相关操作的演示。</p><p>首先，我们先开启分桶功能，命令如下所示。</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; set hive.enforce.bucketing &#x3D; true;</span><br><span class="line">&#x2F;&#x2F;由于HQL最终会转成MR程序，所以分桶数与ReduceTask数保持一致，</span><br><span class="line">&#x2F;&#x2F;从而产生相应的文件个数</span><br><span class="line">hive&gt; set mapreduce.job.reduces&#x3D;4;</span><br></pre></td></tr></table></figure><p></p><p>其次，创建桶表，语法格式如下所示：</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create table stu_buck(Sno int,Sname string,Sex string,Sage int,Sdept string) clustered by(Sno) into 4 buckets row format delimited fields terminated by &#39;,&#39;;</span><br></pre></td></tr></table></figure><p></p><p>执行上述语句后，桶表 stu_buck 创建完成，并且以学生编号（Sno）分为 4 个桶，以 “，” 为分隔符的桶表。</p><p>再次，在 HDFS 的 /stu/ 目录下已有结构化数据文件 student.txt，我们需要将 student.txt 文件的复制到 /hivedata 目录下。然后，加载数据到桶表中，由于分桶表加载数据时，不能使用 Load Data 方式导入数据（原因在于该 Load Data 本质上是对数据文件进行复制或移动到 Hive 表所对应的地址中），因此在分桶表导入数据时需要创建临时的 student 表，该表与 stu_buck 表的字段必须一致，语法格式如下所示：</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create table student_tmp(Sno int,Sname string,Sex string,Sage int,Sdept string) </span><br><span class="line">row format delimited fields terminated by &#39;,&#39;;</span><br></pre></td></tr></table></figure><p></p><p>依次，加载数据至 student 表，语法格式如下所示</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; load data local inpath &#39;&#x2F;hivedata&#x2F;student.txt&#39; into table student_tmp;</span><br></pre></td></tr></table></figure><p></p><p>最后，将数据导入 stu_buck 表，语法格式如下所示：</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; insert overwrite table stu_buck select * from student_tmp cluster by(Sno);</span><br></pre></td></tr></table></figure><p></p><p>总体来说，分桶表是把表所映射的结构化数据分得更细致，且分桶规则与 MapReduce 分区规则一致，Hive 采用对目标列值进行哈希运算，得到哈希值再与桶个数取模的方式决定数据的归并，从而看出 Hive 与 MapReduce 存在紧密联系。使用分桶可以提高查询效率，例如执行 Join 操作时，两个表有相同的列字段，如果对这两张表都采取了分桶操作，那么就可以减少 Join 操作时的数据量，从而提高查询效率。它还能够在处理大规模数据集时，选择小部分数据集进行抽样运算，从而减少资源浪费。</p><div class="tags"><a href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8/" rel="tag"><i class="ic i-tag"></i> 大数据技术与应用</a> <a href="/tags/Hive/" rel="tag"><i class="ic i-tag"></i> Hive</a></div></div><footer><div class="meta"><span class="item"><span class="icon"><i class="ic i-calendar-check"></i> </span><span class="text">更新于</span> <time title="修改时间：2021-12-02 00:51:09" itemprop="dateModified" datetime="2021-12-02T00:51:09+08:00">2021-12-02</time> </span><span id="2021/12/02/在线课程/大数据技术与应用/Hive数据仓库操作/" class="item leancloud_visitors" data-flag-title="Hive数据类型操作" title="阅读次数"><span class="icon"><i class="ic i-eye"></i> </span><span class="text">阅读次数</span> <span class="leancloud-visitors-count"></span> <span class="text">次</span></span></div><div class="reward"><button><i class="ic i-heartbeat"></i> 赞赏</button><p>请我喝[茶]~(￣▽￣)~*</p><div id="qr"><div><img data-src="/images/wechatpay.png" alt="Hunter Yu 微信支付"><p>微信支付</p></div><div><img data-src="/images/alipay.png" alt="Hunter Yu 支付宝"><p>支付宝</p></div></div></div><div id="copyright"><ul><li class="author"><strong>本文作者： </strong>Hunter Yu <i class="ic i-at"><em>@</em></i>余心所善，九死未悔！</li><li class="link"><strong>本文链接：</strong> <a href="http://example.com/2021/12/02/%E5%9C%A8%E7%BA%BF%E8%AF%BE%E7%A8%8B/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8/Hive%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%93%8D%E4%BD%9C/" title="Hive数据类型操作">http://example.com/2021/12/02/在线课程/大数据技术与应用/Hive数据仓库操作/</a></li><li class="license"><strong>版权声明： </strong>本站所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpo"><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</span> 许可协议。转载请注明出处！</li></ul></div></footer></article></div><div class="post-nav"><div class="item left"><a href="/2021/11/30/%E5%9C%A8%E7%BA%BF%E8%AF%BE%E7%A8%8B/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8/Hive%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/" itemprop="url" rel="prev" data-background-image="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;mw690&#x2F;6833939bly1gicitzannuj20zk0m8b29.jpg" title="Hive安装教程"><span class="type">上一篇</span> <span class="category"><i class="ic i-flag"></i> Hive安装教程</span><h3>Hive安装教程</h3></a></div><div class="item right"></div></div><div class="wrap" id="comments"></div></div><div id="sidebar"><div class="inner"><div class="panels"><div class="inner"><div class="contents panel pjax" data-title="文章目录"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#hive%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E6%93%8D%E4%BD%9C"><span class="toc-number">1.</span> <span class="toc-text">Hive 数据类型操作</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#hive%E5%86%85%E9%83%A8%E8%A1%A8%E6%93%8D%E4%BD%9C"><span class="toc-number">1.1.</span> <span class="toc-text">Hive 内部表操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%92%88%E5%AF%B9%E5%9F%BA%E6%9C%AC%E7%B1%BB%E5%9E%8B%E5%BB%BA%E8%A1%A8"><span class="toc-number">1.1.1.</span> <span class="toc-text">针对基本类型建表</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%92%88%E5%AF%B9%E5%A4%8D%E6%9D%82%E7%B1%BB%E5%9E%8B%E5%BB%BA%E8%A1%A8"><span class="toc-number">1.1.2.</span> <span class="toc-text">针对复杂类型建表</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#hive%E5%A4%96%E9%83%A8%E8%A1%A8%E6%93%8D%E4%BD%9C"><span class="toc-number">1.2.</span> <span class="toc-text">Hive 外部表操作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#hive%E5%88%86%E5%8C%BA%E8%A1%A8%E6%93%8D%E4%BD%9C"><span class="toc-number">1.3.</span> <span class="toc-text">Hive 分区表操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#hive%E6%99%AE%E9%80%9A%E5%88%86%E5%8C%BA"><span class="toc-number">1.3.1.</span> <span class="toc-text">Hive 普通分区</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#hive%E5%8A%A8%E6%80%81%E5%88%86%E5%8C%BA"><span class="toc-number">1.3.2.</span> <span class="toc-text">Hive 动态分区</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#hive%E6%A1%B6%E8%A1%A8%E6%93%8D%E4%BD%9C"><span class="toc-number">1.4.</span> <span class="toc-text">Hive 桶表操作</span></a></li></ol></li></ol></div><div class="related panel pjax" data-title="系列文章"><ul><li><a href="/2021/11/30/%E5%9C%A8%E7%BA%BF%E8%AF%BE%E7%A8%8B/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8/Hive%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/" rel="bookmark" title="Hive安装教程">Hive安装教程</a></li><li class="active"><a href="/2021/12/02/%E5%9C%A8%E7%BA%BF%E8%AF%BE%E7%A8%8B/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8/Hive%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%93%8D%E4%BD%9C/" rel="bookmark" title="Hive数据类型操作">Hive数据类型操作</a></li></ul></div><div class="overview panel" data-title="站点概览"><div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="image" itemprop="image" alt="Hunter Yu" data-src="/images/avatar.jpg"><p class="name" itemprop="name">Hunter Yu</p><div class="description" itemprop="description"></div></div><nav class="state"><div class="item posts"><a href="/archives/"><span class="count">15</span> <span class="name">文章</span></a></div><div class="item categories"><a href="/categories/"><span class="count">24</span> <span class="name">分类</span></a></div><div class="item tags"><a href="/tags/"><span class="count">3</span> <span class="name">标签</span></a></div></nav><div class="social"><span class="exturl item github" data-url="aHR0cHM6Ly9naXRodWIuY29tL0dvRmlzaGVy" title="https:&#x2F;&#x2F;github.com&#x2F;GoFisher"><i class="ic i-github"></i></span> <span class="exturl item music" data-url="aHR0cHM6Ly9tdXNpYy4xNjMuY29tLyMvdXNlci9ob21lP2lkPTExOTc1MTQxMg==" title="https:&#x2F;&#x2F;music.163.com&#x2F;#&#x2F;user&#x2F;home?id&#x3D;119751412"><i class="ic i-cloud-music"></i></span> <span class="exturl item weibo" data-url="aHR0cHM6Ly93ZWliby5jb20vODA5MDI4NDYvaG9tZT93dnI9NQ==" title="https:&#x2F;&#x2F;weibo.com&#x2F;80902846&#x2F;home?wvr&#x3D;5"><i class="ic i-weibo"></i></span> <span class="exturl item about" data-url="aHR0cHM6Ly9zcGFjZS5iaWxpYmlsaS5jb20vODQyNTA5OQ==" title="https:&#x2F;&#x2F;space.bilibili.com&#x2F;8425099"><i class="ic i-address-card"></i></span></div><ul class="menu"><li class="item"><a href="/" rel="section"><i class="ic i-home"></i>首页</a></li><li class="item"><a href="/about/" rel="section"><i class="ic i-user"></i>关于</a></li><li class="item dropdown"><a href="javascript:void(0);"><i class="ic i-feather"></i>文章</a><ul class="submenu"><li class="item"><a href="/archives/" rel="section"><i class="ic i-list-alt"></i>归档</a></li><li class="item"><a href="/categories/" rel="section"><i class="ic i-th"></i>分类</a></li><li class="item"><a href="/tags/" rel="section"><i class="ic i-tags"></i>标签</a></li></ul></li><li class="item"><a href="/links/" rel="section"><i class="ic i-magic"></i>links</a></li></ul></div></div></div><ul id="quick"><li class="prev pjax"></li><li class="up"><i class="ic i-arrow-up"></i></li><li class="down"><i class="ic i-arrow-down"></i></li><li class="next pjax"></li><li class="percent"></li></ul></div></div><div class="dimmer"></div></div></main><footer id="footer"><div class="inner"><div class="widgets"><div class="rpost pjax"><h2>随机文章</h2><ul><li class="item"><div class="breadcrumb"><a href="/categories/%E5%9C%A8%E7%BA%BF%E8%AF%BE%E7%A8%8B/" title="分类于 在线课程">在线课程</a> <i class="ic i-angle-right"></i> <a href="/categories/%E5%9C%A8%E7%BA%BF%E8%AF%BE%E7%A8%8B/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8/" title="分类于 大数据技术与应用">大数据技术与应用</a> <i class="ic i-angle-right"></i> <a href="/categories/%E5%9C%A8%E7%BA%BF%E8%AF%BE%E7%A8%8B/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8/Hadoop%E5%B9%B3%E5%8F%B0%E6%90%AD%E5%BB%BA%E4%BB%BB%E5%8A%A1%E8%83%8C%E6%99%AF/" title="分类于 Hadoop平台搭建任务背景">Hadoop平台搭建任务背景</a></div><span><a href="/2020/11/20/%E5%9C%A8%E7%BA%BF%E8%AF%BE%E7%A8%8B/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8/Hadoop%E5%AE%89%E8%A3%85%E4%BB%BB%E5%8A%A1%E8%83%8C%E6%99%AF/" title="Hadoop平台搭建任务背景">Hadoop平台搭建任务背景</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/%E5%9C%A8%E7%BA%BF%E8%AF%BE%E7%A8%8B/" title="分类于 在线课程">在线课程</a> <i class="ic i-angle-right"></i> <a href="/categories/%E5%9C%A8%E7%BA%BF%E8%AF%BE%E7%A8%8B/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8/" title="分类于 大数据技术与应用">大数据技术与应用</a> <i class="ic i-angle-right"></i> <a href="/categories/%E5%9C%A8%E7%BA%BF%E8%AF%BE%E7%A8%8B/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8/5-%E6%97%B6%E9%97%B4%E5%90%8C%E6%AD%A5%E6%9C%8D%E5%8A%A1%E3%80%81%E5%85%B3%E9%97%AD%E9%98%B2%E7%81%AB%E5%A2%99%E3%80%81IP%E6%98%A0%E5%B0%84/" title="分类于 5.时间同步服务、关闭防火墙、IP映射">5.时间同步服务、关闭防火墙、IP映射</a></div><span><a href="/2020/12/02/%E5%9C%A8%E7%BA%BF%E8%AF%BE%E7%A8%8B/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8/%E6%97%B6%E9%97%B4%E5%90%8C%E6%AD%A5%E6%9C%8D%E5%8A%A1%E3%80%81%E5%85%B3%E9%97%AD%E9%98%B2%E7%81%AB%E5%A2%99%E3%80%81IP%E6%98%A0%E5%B0%84/" title="4.时间同步服务、关闭防火墙、IP映射">4.时间同步服务、关闭防火墙、IP映射</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/%E5%9C%A8%E7%BA%BF%E8%AF%BE%E7%A8%8B/" title="分类于 在线课程">在线课程</a> <i class="ic i-angle-right"></i> <a href="/categories/%E5%9C%A8%E7%BA%BF%E8%AF%BE%E7%A8%8B/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8/" title="分类于 大数据技术与应用">大数据技术与应用</a> <i class="ic i-angle-right"></i> <a href="/categories/%E5%9C%A8%E7%BA%BF%E8%AF%BE%E7%A8%8B/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8/7-%E5%90%AF%E5%8A%A8Hadoop%E5%B9%B6%E6%9F%A5%E7%9C%8B/" title="分类于 7.启动Hadoop并查看">7.启动Hadoop并查看</a></div><span><a href="/2020/12/02/%E5%9C%A8%E7%BA%BF%E8%AF%BE%E7%A8%8B/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8/%E5%90%AF%E5%8A%A8Hadoop%E9%9B%86%E7%BE%A4/" title="7.启动Hadoop并查看">7.启动Hadoop并查看</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/%E5%9C%A8%E7%BA%BF%E8%AF%BE%E7%A8%8B/" title="分类于 在线课程">在线课程</a> <i class="ic i-angle-right"></i> <a href="/categories/%E5%9C%A8%E7%BA%BF%E8%AF%BE%E7%A8%8B/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8/" title="分类于 大数据技术与应用">大数据技术与应用</a> <i class="ic i-angle-right"></i> <a href="/categories/%E5%9C%A8%E7%BA%BF%E8%AF%BE%E7%A8%8B/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8/5-%E5%85%8B%E9%9A%86%E8%8A%82%E7%82%B9%E9%85%8D%E7%BD%AE%E4%BF%AE%E6%94%B9/" title="分类于 5.克隆节点配置修改">5.克隆节点配置修改</a></div><span><a href="/2020/12/02/%E5%9C%A8%E7%BA%BF%E8%AF%BE%E7%A8%8B/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8/%E5%85%8B%E9%9A%86%E8%8A%82%E7%82%B9%E9%85%8D%E7%BD%AE%E4%BF%AE%E6%94%B9/" title="5.克隆节点配置修改">5.克隆节点配置修改</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/%E5%9C%A8%E7%BA%BF%E8%AF%BE%E7%A8%8B/" title="分类于 在线课程">在线课程</a> <i class="ic i-angle-right"></i> <a href="/categories/%E5%9C%A8%E7%BA%BF%E8%AF%BE%E7%A8%8B/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8/" title="分类于 大数据技术与应用">大数据技术与应用</a> <i class="ic i-angle-right"></i> <a href="/categories/%E5%9C%A8%E7%BA%BF%E8%AF%BE%E7%A8%8B/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8/Hive%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/" title="分类于 Hive安装教程">Hive安装教程</a></div><span><a href="/2021/12/02/%E5%9C%A8%E7%BA%BF%E8%AF%BE%E7%A8%8B/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8/Hive%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%93%8D%E4%BD%9C/" title="Hive数据类型操作">Hive数据类型操作</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/%E5%9C%A8%E7%BA%BF%E8%AF%BE%E7%A8%8B/" title="分类于 在线课程">在线课程</a> <i class="ic i-angle-right"></i> <a href="/categories/%E5%9C%A8%E7%BA%BF%E8%AF%BE%E7%A8%8B/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8/" title="分类于 大数据技术与应用">大数据技术与应用</a> <i class="ic i-angle-right"></i> <a href="/categories/%E5%9C%A8%E7%BA%BF%E8%AF%BE%E7%A8%8B/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8/6-%E9%85%8D%E7%BD%AE%E5%85%8D%E5%AF%86%E7%A0%81%E7%99%BB%E5%BD%95/" title="分类于 6.配置免密码登录">6.配置免密码登录</a></div><span><a href="/2020/12/02/%E5%9C%A8%E7%BA%BF%E8%AF%BE%E7%A8%8B/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8/%E9%85%8D%E7%BD%AE%E5%85%8D%E5%AF%86%E7%A0%81%E7%99%BB%E5%BD%95/" title="6.配置免密码登录">6.配置免密码登录</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/%E5%9C%A8%E7%BA%BF%E8%AF%BE%E7%A8%8B/" title="分类于 在线课程">在线课程</a> <i class="ic i-angle-right"></i> <a href="/categories/%E5%9C%A8%E7%BA%BF%E8%AF%BE%E7%A8%8B/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8/" title="分类于 大数据技术与应用">大数据技术与应用</a> <i class="ic i-angle-right"></i> <a href="/categories/%E5%9C%A8%E7%BA%BF%E8%AF%BE%E7%A8%8B/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8/Hive%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/" title="分类于 Hive安装教程">Hive安装教程</a></div><span><a href="/2021/11/30/%E5%9C%A8%E7%BA%BF%E8%AF%BE%E7%A8%8B/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8/Hive%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/" title="Hive安装教程">Hive安装教程</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/%E5%9C%A8%E7%BA%BF%E8%AF%BE%E7%A8%8B/" title="分类于 在线课程">在线课程</a> <i class="ic i-angle-right"></i> <a href="/categories/%E5%9C%A8%E7%BA%BF%E8%AF%BE%E7%A8%8B/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8/" title="分类于 大数据技术与应用">大数据技术与应用</a> <i class="ic i-angle-right"></i> <a href="/categories/%E5%9C%A8%E7%BA%BF%E8%AF%BE%E7%A8%8B/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8/3-%E5%AE%89%E8%A3%85Jdk%E5%8F%8AHadoop/" title="分类于 3.安装Jdk及Hadoop">3.安装Jdk及Hadoop</a></div><span><a href="/2020/12/02/%E5%9C%A8%E7%BA%BF%E8%AF%BE%E7%A8%8B/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8/%E5%AE%89%E8%A3%85JDK%E5%8F%8AHadoop/" title="3.安装Jdk及Hadoop">3.安装Jdk及Hadoop</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/%E7%9F%A5%E8%AF%86%E6%8B%93%E5%B1%95/" title="分类于 知识拓展">知识拓展</a> <i class="ic i-angle-right"></i> <a href="/categories/%E7%9F%A5%E8%AF%86%E6%8B%93%E5%B1%95/%E8%B6%A3%E5%91%B3%E6%9D%82%E8%B0%88/" title="分类于 趣味杂谈">趣味杂谈</a> <i class="ic i-angle-right"></i> <a href="/categories/%E7%9F%A5%E8%AF%86%E6%8B%93%E5%B1%95/%E8%B6%A3%E5%91%B3%E6%9D%82%E8%B0%88/%E5%AE%9E%E8%AE%AD%E5%91%A8%E7%9F%A5%E8%AF%86%E7%82%B9%E8%A1%A5%E5%85%85/" title="分类于 实训周知识点补充">实训周知识点补充</a></div><span><a href="/2020/11/20/%E7%9F%A5%E8%AF%86%E6%8B%93%E5%B1%95/%E8%B6%A3%E5%91%B3%E6%9D%82%E8%B0%88/%E5%AE%9E%E8%AE%AD%E5%91%A8%E7%9F%A5%E8%AF%86%E7%82%B9%E8%A1%A5%E5%85%85/" title="实训周知识点补充">实训周知识点补充</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/%E6%97%A5%E5%B8%B8%E7%A7%AF%E7%B4%AF/" title="分类于 日常积累">日常积累</a> <i class="ic i-angle-right"></i> <a href="/categories/%E6%97%A5%E5%B8%B8%E7%A7%AF%E7%B4%AF/%E6%AF%8F%E5%91%A8%E4%B8%80%E7%BB%83/" title="分类于 每周一练">每周一练</a> <i class="ic i-angle-right"></i> <a href="/categories/%E6%97%A5%E5%B8%B8%E7%A7%AF%E7%B4%AF/%E6%AF%8F%E5%91%A8%E4%B8%80%E7%BB%83/demo2/" title="分类于 demo2">demo2</a></div><span><a href="/2020/11/14/%E6%97%A5%E5%B8%B8%E7%A7%AF%E7%B4%AF/%E6%AF%8F%E5%91%A8%E4%B8%80%E7%BB%83/demo2/" title="demo2">demo2</a></span></li></ul></div><div><h2>最新评论</h2><ul class="leancloud-recent-comment"></ul></div></div><div class="status"><div class="copyright">&copy; 2020 – <span itemprop="copyrightYear">2021</span> <span class="with-love"><i class="ic i-sakura rotate"></i> </span><span class="author" itemprop="copyrightHolder">Hunter Yu @ Yu's Sky</span></div><div class="count"><span class="post-meta-item-icon"><i class="ic i-chart-area"></i> </span><span title="站点总字数">69k 字</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="ic i-coffee"></i> </span><span title="站点阅读时长">1:03</span></div><div class="powered-by">基于 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & Theme.<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FtZWhpbWUvaGV4by10aGVtZS1zaG9rYQ==">Shoka</span></div></div></div></footer></div><script data-config type="text/javascript">var LOCAL={path:"2021/12/02/在线课程/大数据技术与应用/Hive数据仓库操作/",favicon:{show:"（●´3｀●）やれやれだぜ",hide:"(´Д｀)大変だ！"},search:{placeholder:"文章搜索",empty:"关于 「 ${query} 」，什么也没搜到",stats:"${time} ms 内找到 ${hits} 条结果"},valine:!0,fancybox:!0,copyright:'复制成功，转载请遵守 <i class="ic i-creative-commons"></i>BY-NC-SA 协议。',ignores:[function(e){return e.includes("#")},function(e){return new RegExp(LOCAL.path+"$").test(e)}]}</script><script src="https://cdn.polyfill.io/v2/polyfill.js"></script><script src="//cdn.jsdelivr.net/combine/npm/pace-js@1.0.2/pace.min.js,npm/pjax@0.2.8/pjax.min.js,npm/whatwg-fetch@3.4.0/dist/fetch.umd.min.js,npm/animejs@3.2.0/lib/anime.min.js,npm/algoliasearch@4/dist/algoliasearch-lite.umd.js,npm/instantsearch.js@4/dist/instantsearch.production.min.js,npm/lozad@1/dist/lozad.min.js,npm/quicklink@2/dist/quicklink.umd.js"></script><script src="/js/app.js?v=0.2.5"></script></body></html><!-- rebuild by hrmmi -->