{
    "version": "https://jsonfeed.org/version/1",
    "title": "null • All posts by \"hive安装教程\" category",
    "description": "",
    "home_page_url": "http://example.com",
    "items": [
        {
            "id": "http://example.com/2021/12/02/%E5%9C%A8%E7%BA%BF%E8%AF%BE%E7%A8%8B/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8/Hive%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%93%8D%E4%BD%9C/",
            "url": "http://example.com/2021/12/02/%E5%9C%A8%E7%BA%BF%E8%AF%BE%E7%A8%8B/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8/Hive%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%93%8D%E4%BD%9C/",
            "title": "Hive数据类型操作",
            "date_published": "2021-12-01T16:49:21.872Z",
            "content_html": "<h1 id=\"hive数据类型操作\"><a class=\"anchor\" href=\"#hive数据类型操作\">#</a> Hive 数据类型操作</h1>\n<h2 id=\"hive内部表操作\"><a class=\"anchor\" href=\"#hive内部表操作\">#</a> Hive 内部表操作</h2>\n<h3 id=\"针对基本类型建表\"><a class=\"anchor\" href=\"#针对基本类型建表\">#</a> 针对基本类型建表</h3>\n<p>首先，在 master 机器的 <code>/export/data</code>  目录下创建 hivedata 目录，在改文件夹下创建 <code>user.txt</code> ，并添加如下数据内容：</p>\n<p><figure class=\"highlight txt\"><figcaption><span>测试数据内容</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1,allen,18</span><br><span class=\"line\">2,tom,23</span><br><span class=\"line\">3,jerry,28</span><br></pre></td></tr></table></figure></p>\n<p>具体操作如下：</p>\n<p><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@master ~]<span class=\"comment\"># mkdir -p /export/data/hivedata</span></span><br><span class=\"line\">[root@master ~]<span class=\"comment\"># cd /export/data/hivedata/</span></span><br><span class=\"line\">[root@master hivedata]<span class=\"comment\"># vi user.txt </span></span><br><span class=\"line\">[root@master hivedata]<span class=\"comment\"># cat user.txt </span></span><br><span class=\"line\">1,allen,18</span><br><span class=\"line\">2,tom,23</span><br><span class=\"line\">3,jerry,28</span><br></pre></td></tr></table></figure></p>\n<p>启动 hadoop 和 hive, 具体如下操作</p>\n<p><figure class=\"highlight bash\"><figcaption><span>bash代码</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@master data]<span class=\"comment\"># sh /usr/local/hadoop-2.6.5/sbin/start-all.sh</span></span><br><span class=\"line\">[root@master data]<span class=\"comment\"># sh /export/servers/apache-hive-1.2.1-bin/bin/hive</span></span><br></pre></td></tr></table></figure></p>\n<p><img data-src=\"https://gitee.com/gofisher/typora_pic/raw/master/images/image-20211201232425173.png\" alt=\"image-20211201232425173\" /></p>\n<p>在 hive 界面输入 <code>set hive.cli.print.current.db=true</code>  设置显示当前使用数据库，然后创建 <code>zjdf</code>  数据库，创建数据库后可以使用 <code>show databases</code>  命令进行查看，如果已经存在，可以使用 <code>drop database database_name cascade</code>  命令强制性删除，如果有表格，一定要添加 <code>cascade</code>  参数，用 <code>use</code>  切换到这个新建的数据库，并创建内部表 <code>t_user</code> , 具体操作如下：</p>\n<p><figure class=\"highlight plain\"><figcaption><span>hive界面命令</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hive&gt; set hive.cli.print.current.db&#x3D;true;</span><br><span class=\"line\">hive (default)&gt; show databases;</span><br><span class=\"line\">OK</span><br><span class=\"line\">default</span><br><span class=\"line\">Time taken: 0.023 seconds, Fetched: 1 row(s)</span><br><span class=\"line\">hive (default)&gt; create database zjdf;</span><br><span class=\"line\">OK</span><br><span class=\"line\">Time taken: 0.182 seconds、  </span><br><span class=\"line\">hive (default)&gt; show databases;</span><br><span class=\"line\">OK</span><br><span class=\"line\">default</span><br><span class=\"line\">zjdf</span><br><span class=\"line\">Time taken: 0.016 seconds, Fetched: 2 row(s)</span><br><span class=\"line\">hive (default)&gt; use zjdf;</span><br><span class=\"line\">OK</span><br><span class=\"line\">Time taken: 0.024 seconds</span><br><span class=\"line\">hive (zjdf)&gt;</span><br></pre></td></tr></table></figure></p>\n<p><img data-src=\"https://gitee.com/gofisher/typora_pic/raw/master/images/image-20211201233838557.png\" alt=\"image-20211201233838557\" /></p>\n<p>针对 hivedata 目录准备的结构化文件 user.txt 先创建一个内部表 t_user，具体示例如下：</p>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hive (zjdf)&gt; create table t_user(id int,name string,age int) ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;,&#39;;</span><br><span class=\"line\">OK</span><br><span class=\"line\">Time taken: 0.561 seconds</span><br><span class=\"line\">hive (zjdf)&gt; show tables;</span><br><span class=\"line\">OK</span><br><span class=\"line\">t_user</span><br><span class=\"line\">Time taken: 0.037 seconds, Fetched: 1 row(s)</span><br><span class=\"line\">hive (zjdf)&gt; </span><br></pre></td></tr></table></figure></p>\n<p>创建成功后，通过 WEB UI 界面打开 Hive 内部表所在 HDFS 路径（默认 /user/hive/warehouse/zidj.db/t_user）进行查看，如下图所示。</p>\n<p><img data-src=\"https://gitee.com/gofisher/typora_pic/raw/master/images/image-20211201234130736.png\" alt=\"image-20211201234130736\" /></p>\n<p>复制 master 终端，然后点击选项卡排列，可以达到左右分屏的效果，如下所示：</p>\n<p><img data-src=\"https://gitee.com/gofisher/typora_pic/raw/master/images/image-20211201234439467.png\" alt=\"image-20211201234439467\" /></p>\n<p>将准备好的 user.txt 移动到 hadoop 对应的 t_user 表的路径中，具体操作如下：</p>\n<p><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@master hivedata]<span class=\"comment\"># hadoop fs -put user.txt /user/hive/warehouse/zjdf.db/t_user</span></span><br><span class=\"line\">21/12/01 10:05:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library <span class=\"keyword\">for</span> your platform... using builtin-java classes <span class=\"built_in\">where</span> applicable</span><br><span class=\"line\">[root@master hivedata]<span class=\"comment\">#</span></span><br></pre></td></tr></table></figure></p>\n<p>刷新 Hadoop 文件系统，如下图所示：</p>\n<p><img data-src=\"https://gitee.com/gofisher/typora_pic/raw/master/images/image-20211201234942510.png\" alt=\"image-20211201234942510\" /></p>\n<p>在 hive 界面中查询 t_user 表数据，发现表中已经存在数据，具体命令如下：</p>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hive (zjdf)&gt; select * from t_user;</span><br><span class=\"line\">OK</span><br><span class=\"line\">1\tallen\t18</span><br><span class=\"line\">2\ttom\t23</span><br><span class=\"line\">3\tjerry\t28</span><br><span class=\"line\">Time taken: 0.476 seconds, Fetched: 3 row(s)</span><br><span class=\"line\">hive (zjdf)&gt;</span><br></pre></td></tr></table></figure></p>\n<p><img data-src=\"https://gitee.com/gofisher/typora_pic/raw/master/images/image-20211201235147810.png\" alt=\"image-20211201235147810\" /></p>\n<h3 id=\"针对复杂类型建表\"><a class=\"anchor\" href=\"#针对复杂类型建表\">#</a> 针对复杂类型建表</h3>\n<p>例如，现有结构化数据文件 student.txt，文件内容如下所示。</p>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1,zhangsan,唱歌:非常喜欢-跳舞:喜欢-游泳:一般般</span><br><span class=\"line\">2,lisi,打游戏:非常喜欢-篮球:不喜欢</span><br></pre></td></tr></table></figure></p>\n<p>通过对 student.txt 文件内容分析得出，可以设计为 3 列字段，即编号、姓名、兴趣，其中编号可以为 int 类型，姓名可以为 string 类型，而兴趣列还需要进一步分隔为 Map 类型，因此在创建 student.txt 文件对应的内部表语句如下所示：</p>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hive (zjdf)&gt; create table t_student(id int,name string,hobby map&lt;string,string&gt;) row format delimited fields terminated by &#39;,&#39;  collection items terminated by &#39;-&#39; map keys terminated by &#39;:&#39;;</span><br><span class=\"line\">OK</span><br><span class=\"line\">Time taken: 0.064 seconds</span><br><span class=\"line\">hive (zjdf)&gt; show tables;</span><br><span class=\"line\">OK</span><br><span class=\"line\">t_student</span><br><span class=\"line\">t_user</span><br><span class=\"line\">Time taken: 0.03 seconds, Fetched: 2 row(s)</span><br><span class=\"line\">hive (zjdf)&gt;</span><br></pre></td></tr></table></figure></p>\n<p>上述建表语句中，通过对 student.txt 文件结构化文件的分析，先通过逗号 “，” 对多个字段 fields 进行分隔；接着，针对 hobby 字段列，通过横线 “-” 进行集合列分隔；最后，再针对每一个爱好，通过冒号 “：” 进行分隔为最终的 “key：value” 形式。执行上述建表语句后，就会在默认的 /user/hive/warehouse/zjdf.db 文件夹下生成一个 t_student 文件夹。如下图所示：</p>\n<p><img data-src=\"https://gitee.com/gofisher/typora_pic/raw/master/images/image-20211201235638917.png\" alt=\"image-20211201235638917\" /></p>\n<p>此时，还必须将前面的结构化文件 student.txt 上传到该文件夹下进行映射，才能生成对应的内部表数据，上传完成后再次查询生成的 t_student 表信息。</p>\n<p><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@master hivedata]<span class=\"comment\"># vi student.txt</span></span><br><span class=\"line\">[root@master hivedata]<span class=\"comment\"># cat student.txt </span></span><br><span class=\"line\">1,zhangsan,唱歌:非常喜欢-跳舞:喜欢-游泳:一般般</span><br><span class=\"line\">2,lisi,打游戏:非常喜欢-篮球:不喜欢</span><br><span class=\"line\">[root@master hivedata]<span class=\"comment\"># hadoop fs -put student.txt /user/hive/warehouse/zjdf.db/t_student</span></span><br><span class=\"line\">21/12/01 10:15:07 WARN util.NativeCodeLoader: Unable to load native-hadoop library <span class=\"keyword\">for</span> your platform... using builtin-java classes <span class=\"built_in\">where</span> applicable</span><br><span class=\"line\">[root@master hivedata]<span class=\"comment\">#</span></span><br></pre></td></tr></table></figure></p>\n<p>再次在 hive 中查询 t_student 数据表格，如下图所示：</p>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hive (zjdf)&gt; select * from t_student;</span><br><span class=\"line\">OK</span><br><span class=\"line\">1\tzhangsan\t&#123;&quot;唱歌&quot;:&quot;非常喜欢&quot;,&quot;跳舞&quot;:&quot;喜欢&quot;,&quot;游泳&quot;:&quot;一般般&quot;&#125;</span><br><span class=\"line\">2\tlisi\t&#123;&quot;打游戏&quot;:&quot;非常喜欢&quot;,&quot;篮球&quot;:&quot;不喜欢&quot;&#125;</span><br><span class=\"line\">Time taken: 0.082 seconds, Fetched: 2 row(s)</span><br><span class=\"line\">hive (zjdf)&gt; </span><br></pre></td></tr></table></figure></p>\n<h2 id=\"hive外部表操作\"><a class=\"anchor\" href=\"#hive外部表操作\">#</a> Hive 外部表操作</h2>\n<p>内部表，即不添加关键字 External，内部表与结构化数据文件要想产生关系映射，那么数据文件就必须在指定的内部表文件夹下，而当遇到大文件的情况时，移动数据文件非常耗时，这就需要我们来创建外部表，因为它不需要移动结构化数据文件。下面我们通过一个小案例来，对外部表进行讲解。</p>\n<p>现有结构化数据文件 student.txt，且数据内容如文件所示。</p>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">95001,李勇,男,20,CS</span><br><span class=\"line\">95002,刘晨,女,19,IS</span><br><span class=\"line\">95003,王敏,女,22,MA</span><br><span class=\"line\">95004,张立,男,19,IS</span><br><span class=\"line\">95005,刘刚,男,18,MA</span><br><span class=\"line\">95006,孙庆,男,23,CS</span><br><span class=\"line\">95007,易思玲,女,19,MA</span><br><span class=\"line\">95008,李娜,女,18,CS</span><br><span class=\"line\">95009,梦圆圆,女,18,MA</span><br><span class=\"line\">95010,孔小涛,男,19,CS</span><br><span class=\"line\">95011,包小柏,男,18,MA</span><br><span class=\"line\">95012,孙花,女,20,CS</span><br><span class=\"line\">95013,冯伟,男,21,CS</span><br><span class=\"line\">95014,王小丽,女,19,CS</span><br><span class=\"line\">95015,王君,男,18,MA</span><br><span class=\"line\">95016,钱国,男,21,MA</span><br><span class=\"line\">95017,王风娟,女,18,IS</span><br><span class=\"line\">95018,王一,女,19,IS</span><br><span class=\"line\">95019,邢小丽,女,19,IS</span><br><span class=\"line\">95020,赵钱,男,21,IS</span><br><span class=\"line\">95021,周二,男,17,MA</span><br><span class=\"line\">95022,郑明,男,20,MA</span><br></pre></td></tr></table></figure></p>\n<p>首先，我们将 student.txt 文件上传至 HDFS 上的 /stu 路径下，用来模拟生产环境下的数据文件，具体命令如下所示：</p>\n<p><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@master ~]<span class=\"comment\"># vi student.txt </span></span><br><span class=\"line\">[root@master ~]<span class=\"comment\"># hadoop fs -mkdir /stu</span></span><br><span class=\"line\">21/12/01 10:21:27 WARN util.NativeCodeLoader: Unable to load native-hadoop library <span class=\"keyword\">for</span> your platform... using builtin-java classes <span class=\"built_in\">where</span> applicable</span><br><span class=\"line\">[root@master ~]<span class=\"comment\"># hadoop fs -put student.txt /stu</span></span><br><span class=\"line\">21/12/01 10:22:05 WARN util.NativeCodeLoader: Unable to load native-hadoop library <span class=\"keyword\">for</span> your platform... using builtin-java classes <span class=\"built_in\">where</span> applicable</span><br><span class=\"line\">[root@master ~]<span class=\"comment\">#</span></span><br></pre></td></tr></table></figure></p>\n<p>在 hive 中创建外部表</p>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hive (zjdf)&gt; create external table student_ext(Sno int,Sname string,Sex string,Sage int,Sdept string) row format delimited fields terminated by &#39;,&#39; location &#39;&#x2F;stu&#39;;</span><br><span class=\"line\">OK</span><br><span class=\"line\">Time taken: 0.041 seconds</span><br><span class=\"line\">hive (zjdf)&gt; show tables;</span><br><span class=\"line\">OK</span><br><span class=\"line\">student_ext</span><br><span class=\"line\">t_student</span><br><span class=\"line\">t_user</span><br><span class=\"line\">Time taken: 0.027 seconds, Fetched: 3 row(s)</span><br><span class=\"line\">hive (zjdf)&gt; </span><br></pre></td></tr></table></figure></p>\n<p>从结果发现，student_ext 外部表已经创建成功，最后，HQL（HiveQL）对数据表的内容的查看、增加、删除以及修改的语句均与 SQL 语句一致。下面以查看数据表内容为例进行演示，具体语法如下所示：</p>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hive (zjdf)&gt; select * from student_ext;</span><br></pre></td></tr></table></figure></p>\n<p>结果如下</p>\n<p><img data-src=\"https://gitee.com/gofisher/typora_pic/raw/master/images/image-20211202001146733.png\" alt=\"image-20211202001146733\" /></p>\n<p><strong>小提示：Hive 创建内部表时，会将数据移动到数据库指向的路径；创建外部表时，仅记录数据所在的路径，不会对数据的位置做任何改变。在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据</strong></p>\n<h2 id=\"hive分区表操作\"><a class=\"anchor\" href=\"#hive分区表操作\">#</a> Hive 分区表操作</h2>\n<p>分区表是按照属性在文件夹层面给文件更好的管理，实际上就是对应一个 HDFS 文件系统上的独立文件夹，该文件夹下是该分区所有的数据文件。Hive 中的分区就是分目录，把一个大的数据集根据业务需要分割成小的数据集。在查询时通过 WHERE 子句中的表达式选择查询指定的分区，这样的查询效率会提高很多。Hive 分区表一共有两种，分别为普通分区和动态分区，我们下面就要对其分别进行介绍。</p>\n<h3 id=\"hive普通分区\"><a class=\"anchor\" href=\"#hive普通分区\">#</a> Hive 普通分区</h3>\n<p>创建分区表分为两种，一种是单分区，也就是说在表文件夹目录下只有一级文件夹目录。另外一种是多分区，表文件夹下出现多文件夹嵌套模式，现在我们只针对单分区进行详解，若想学习多分区可以参考官网的官方文档。</p>\n<p>现有结构化数据文件 user_p.txt，文件中的数据内容如文件所示。</p>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1,allen</span><br><span class=\"line\">2,tom</span><br><span class=\"line\">3,jerry</span><br></pre></td></tr></table></figure></p>\n<p><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@master ~]<span class=\"comment\"># mkdir -p /export/data/hivedata/</span></span><br><span class=\"line\">[root@master ~]<span class=\"comment\"># cd /export/data/hivedata/</span></span><br><span class=\"line\">[root@master hivedata]<span class=\"comment\"># ll</span></span><br><span class=\"line\">total 8</span><br><span class=\"line\">-rw-r--r--. 1 root root 109 Dec  1 10:14 student.txt</span><br><span class=\"line\">-rw-r--r--. 1 root root  31 Dec  1 09:43 user.txt</span><br><span class=\"line\">[root@master hivedata]<span class=\"comment\"># vi user_p.txt</span></span><br><span class=\"line\">[root@master hivedata]<span class=\"comment\">#</span></span><br></pre></td></tr></table></figure></p>\n<p>首先，创建分区表。语法格式如下所示：</p>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hive (zjdf)&gt; create table t_user_p(id int, name string) partitioned by (country string) row format delimited fields terminated by &#39;,&#39;;</span><br></pre></td></tr></table></figure></p>\n<p><img data-src=\"https://gitee.com/gofisher/typora_pic/raw/master/images/image-20211202002144514.png\" alt=\"image-20211202002144514\" /></p>\n<p>其次，加载数据是将数据文件移动到与 Hive 表对应的位置，从本地（Linux）复制或移动到 HDFS 文件系统的操作。由于分区表在映射数据时不能使用 Hadoop 命令移动文件，需要使用 Load 命令，其语法格式如下所示：</p>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">LOAD DATA [LOCAL] INPATH &#39;filepath&#39; [OVERWRITE] </span><br><span class=\"line\">INTO TABLE table_name [PARTITION (partcol1&#x3D;val1, partcol2&#x3D;val2 ...)]</span><br></pre></td></tr></table></figure></p>\n<p>Load Data 是 HQL 固定的数据装载语句，下面针对部分关键字进行讲解。</p>\n<ul>\n<li>Filepath：它可以引用一个文件（在这种情况下，Hive 将文件移动到表所对应的目录中），或者它可以是一个目录（在这种情况下，Hive 将把该目录中的所有文件移动到表所对应的目录中）。它可以是相对路径、绝对路径以及完整的 URI。</li>\n<li>Local：如果指定了 Local 键字，Load 命令将在本地文件系统（Hive 服务启动方）中查找文件路径，将其复制到对应的 HDFS 路径下；如果没有指定 Local 关键字，它将会从 HDFS 中移动数据文件至对应的表路径下。</li>\n<li>Overwrite：如果使用了 Overwrite 关键字，当加载数据时目标表或分区中的内容会被删除，然后再将 filepath 指向的文件或目录中的内容添加到表或分区中。简单的说就是覆盖表中已有数据；若不添加该关键字，则表示追加数据内容。</li>\n</ul>\n<p>加载数据操作的语法格式如下所示：</p>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hive (zjdf)&gt; load data local inpath &#39;&#x2F;export&#x2F;data&#x2F;hivedata&#x2F;user_p.txt&#39; into table t_user_p partition(country&#x3D;&#39;USA&#39;);</span><br><span class=\"line\">Loading data to table zjdf.t_user_p partition (country&#x3D;USA)</span><br><span class=\"line\">Partition zjdf.t_user_p&#123;country&#x3D;USA&#125; stats: [numFiles&#x3D;1, numRows&#x3D;0, totalSize&#x3D;22, rawDataSize&#x3D;0]</span><br><span class=\"line\">OK</span><br><span class=\"line\">Time taken: 0.618 seconds</span><br><span class=\"line\">hive (zjdf)&gt;</span><br></pre></td></tr></table></figure></p>\n<p>从上述语句看出，Load Data 表示装载数据，Inpath 表示数据文件所在的本地系统路径，partition（country=‘USA’）为指定的分区，它需要与建表时设置的分区字段保持一致。执行完上述命令后，查看表内容的数据，效果如图所示：</p>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hive (zjdf)&gt; select * from t_user_p;</span><br><span class=\"line\">OK</span><br><span class=\"line\">1\tallen\tUSA</span><br><span class=\"line\">2\ttom\tUSA</span><br><span class=\"line\">3\tjerry\tUSA</span><br><span class=\"line\">Time taken: 0.274 seconds, Fetched: 3 row(s)</span><br><span class=\"line\">hive (zjdf)&gt; </span><br></pre></td></tr></table></figure></p>\n<h3 id=\"hive动态分区\"><a class=\"anchor\" href=\"#hive动态分区\">#</a> Hive 动态分区</h3>\n<p>上面介绍了 Hive 普通分区的创建和 Load 命令加载数据的操作。在默认情况下，我们加载数据时，需要手动的设置分区字段，并且针对一个分区就要写一个插入语句。如果源数据量很大时（例如，现有许多日志文件，要求按照日期作为分区字段，在插入数据的时候无法手动的添加分区），就可以利用 Hive 提供的动态分区，可以简化插入数据时的繁琐操作，若想实现动态分区，则需要开启动态分区功能，具体命令如下所示：</p>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hive&gt; set hive.exec.dynamic.partition&#x3D;true;</span><br><span class=\"line\">hive&gt; set hive.exec.dynamic.partition.mode&#x3D;nonstrict;</span><br></pre></td></tr></table></figure></p>\n<p>Hive 默认是不支持动态分区的，因此 hive.exec.dynamic.partition 默认值为 false，需要启动动态分区功能，可以将该参数设置为 true；其中 hive.exec.dynamic.partition.mode 的默认值是 strict，表示必须指定至少一个分区为静态分区，将此参数修改为 nonstrict，表示允许所有的分区字段都可以使用动态分区。</p>\n<p>在 Hive 中 insert 语句是用于动态插入数据的，不同的是它主要是结合 select 查询语句使用，且非常适用于动态分区插入数据，语法格式如下所示：</p>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hive&gt; insert overwrite table table_name </span><br><span class=\"line\">partition (partcol1[&#x3D;val1], partcol2[&#x3D;val2] ...) </span><br><span class=\"line\">select_statement FROM from_statement</span><br></pre></td></tr></table></figure></p>\n<p>现有原始表的结构化数据文件 dynamic_partition_table.txt，内容数据如文件所示。</p>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">2018-05-10,ip1</span><br><span class=\"line\">2018-05-10,ip2</span><br><span class=\"line\">2018-06-14,ip3</span><br><span class=\"line\">2018-06-14,ip4</span><br><span class=\"line\">2018-06-15,ip1</span><br><span class=\"line\">2018-06-15,ip2</span><br></pre></td></tr></table></figure></p>\n<p><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@master hivedata]<span class=\"comment\"># vi dynamic_partition_table.txt</span></span><br><span class=\"line\">[root@master hivedata]<span class=\"comment\"># cat dynamic_partition_table.txt </span></span><br><span class=\"line\">2018-05-10,ip1</span><br><span class=\"line\">2018-05-10,ip2</span><br><span class=\"line\">2018-06-14,ip3</span><br><span class=\"line\">2018-06-14,ip4</span><br><span class=\"line\">2018-06-15,ip1</span><br><span class=\"line\">2018-06-15,ip2</span><br><span class=\"line\">[root@master hivedata]<span class=\"comment\">#</span></span><br></pre></td></tr></table></figure></p>\n<p>现在我们通过一个案例进行演示动态分区的数据插入操作。将 dynamic_partition_table 中的数据按照时间（day），插入到目标表 d_p_t 的相应分区中。</p>\n<p>首先，创建原始表。语法格式如下所示：</p>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hive (zjdf)&gt; create table dynamic_partition_table(day string,ip string) row format delimited fields terminated by &quot;,&quot;;</span><br><span class=\"line\">OK</span><br><span class=\"line\">Time taken: 0.05 seconds</span><br><span class=\"line\">hive (zjdf)&gt; show tables;</span><br><span class=\"line\">OK</span><br><span class=\"line\">dynamic_partition_table</span><br><span class=\"line\">student_ext</span><br><span class=\"line\">t_student</span><br><span class=\"line\">t_user</span><br><span class=\"line\">t_user_p</span><br><span class=\"line\">Time taken: 0.033 seconds, Fetched: 5 row(s)</span><br></pre></td></tr></table></figure></p>\n<p>其次，加载数据文件至原始表，语法格式如下所示：</p>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hive (zjdf)&gt; load data local inpath &#39;&#x2F;export&#x2F;data&#x2F;hivedata&#x2F;dynamic_partition_table.txt&#39; into table dynamic_partition_table;</span><br><span class=\"line\">Loading data to table zjdf.dynamic_partition_table</span><br><span class=\"line\">Table zjdf.dynamic_partition_table stats: [numFiles&#x3D;1, totalSize&#x3D;90]</span><br><span class=\"line\">OK</span><br><span class=\"line\">Time taken: 0.234 seconds</span><br><span class=\"line\">hive (zjdf)&gt; select * from dynamic_partition_table;</span><br><span class=\"line\">OK</span><br><span class=\"line\">2018-05-10\tip1</span><br><span class=\"line\">2018-05-10\tip2</span><br><span class=\"line\">2018-06-14\tip3</span><br><span class=\"line\">2018-06-14\tip4</span><br><span class=\"line\">2018-06-15\tip1</span><br><span class=\"line\">2018-06-15\tip2</span><br><span class=\"line\">Time taken: 0.062 seconds, Fetched: 6 row(s)</span><br><span class=\"line\">hive (zjdf)&gt; </span><br></pre></td></tr></table></figure></p>\n<p>再次，创建目标表，语法格式如下所示：</p>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hive (zjdf)&gt; create table d_p_t(ip string) partitioned by (month string,day string);</span><br><span class=\"line\">OK</span><br><span class=\"line\">Time taken: 0.045 seconds</span><br><span class=\"line\">hive (zjdf)&gt; show tables;</span><br><span class=\"line\">OK</span><br><span class=\"line\">d_p_t</span><br><span class=\"line\">dynamic_partition_table</span><br><span class=\"line\">student_ext</span><br><span class=\"line\">t_student</span><br><span class=\"line\">t_user</span><br><span class=\"line\">t_user_p</span><br><span class=\"line\">Time taken: 0.023 seconds, Fetched: 6 row(s)</span><br><span class=\"line\">hive (zjdf)&gt;</span><br></pre></td></tr></table></figure></p>\n<p>依次，动态插入，语法格式如下所示：</p>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hive (zjdf)&gt; insert overwrite table d_p_t partition (month,day) select ip,substr(day,1,7) as month,day from dynamic_partition_table;</span><br></pre></td></tr></table></figure></p>\n<p><img data-src=\"C:/Users/yujz/AppData/Roaming/Typora/typora-user-images/image-20211202004106877.png\" alt=\"image-20211202004106877\" /></p>\n<p>最后，查看目标表中的分区数据，语法格式如下所示：</p>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hive (zjdf)&gt; show partitions d_p_t;</span><br><span class=\"line\">OK</span><br><span class=\"line\">month&#x3D;2018-05&#x2F;day&#x3D;2018-05-10</span><br><span class=\"line\">month&#x3D;2018-06&#x2F;day&#x3D;2018-06-14</span><br><span class=\"line\">month&#x3D;2018-06&#x2F;day&#x3D;2018-06-15</span><br><span class=\"line\">Time taken: 0.449 seconds, Fetched: 3 row(s)</span><br><span class=\"line\">hive (zjdf)&gt;</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"hive桶表操作\"><a class=\"anchor\" href=\"#hive桶表操作\">#</a> Hive 桶表操作</h2>\n<p>为了将表进行更细粒度的范围划分，我们可以创建桶表。桶表，是根据某个属性字段把数据分成几个桶（我们这里设置为 4，默认值是 - 1，可自定义），也就是在文件的层面上把数据分开。下面通过一个案例进行桶表相关操作的演示。</p>\n<p>首先，我们先开启分桶功能，命令如下所示。</p>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hive&gt; set hive.enforce.bucketing &#x3D; true;</span><br><span class=\"line\">&#x2F;&#x2F;由于HQL最终会转成MR程序，所以分桶数与ReduceTask数保持一致，</span><br><span class=\"line\">&#x2F;&#x2F;从而产生相应的文件个数</span><br><span class=\"line\">hive&gt; set mapreduce.job.reduces&#x3D;4;</span><br></pre></td></tr></table></figure></p>\n<p>其次，创建桶表，语法格式如下所示：</p>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hive&gt; create table stu_buck(Sno int,Sname string,Sex string,Sage int,Sdept string) clustered by(Sno) into 4 buckets row format delimited fields terminated by &#39;,&#39;;</span><br></pre></td></tr></table></figure></p>\n<p>执行上述语句后，桶表 stu_buck 创建完成，并且以学生编号（Sno）分为 4 个桶，以 “，” 为分隔符的桶表。</p>\n<p>再次，在 HDFS 的 /stu/ 目录下已有结构化数据文件 student.txt，我们需要将 student.txt 文件的复制到 /hivedata 目录下。然后，加载数据到桶表中，由于分桶表加载数据时，不能使用 Load Data 方式导入数据（原因在于该 Load Data 本质上是对数据文件进行复制或移动到 Hive 表所对应的地址中），因此在分桶表导入数据时需要创建临时的 student 表，该表与 stu_buck 表的字段必须一致，语法格式如下所示：</p>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hive&gt; create table student_tmp(Sno int,Sname string,Sex string,Sage int,Sdept string) </span><br><span class=\"line\">row format delimited fields terminated by &#39;,&#39;;</span><br></pre></td></tr></table></figure></p>\n<p>依次，加载数据至 student 表，语法格式如下所示</p>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hive&gt; load data local inpath &#39;&#x2F;hivedata&#x2F;student.txt&#39; into table student_tmp;</span><br></pre></td></tr></table></figure></p>\n<p>最后，将数据导入 stu_buck 表，语法格式如下所示：</p>\n<p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hive&gt; insert overwrite table stu_buck select * from student_tmp cluster by(Sno);</span><br></pre></td></tr></table></figure></p>\n<p>总体来说，分桶表是把表所映射的结构化数据分得更细致，且分桶规则与 MapReduce 分区规则一致，Hive 采用对目标列值进行哈希运算，得到哈希值再与桶个数取模的方式决定数据的归并，从而看出 Hive 与 MapReduce 存在紧密联系。使用分桶可以提高查询效率，例如执行 Join 操作时，两个表有相同的列字段，如果对这两张表都采取了分桶操作，那么就可以减少 Join 操作时的数据量，从而提高查询效率。它还能够在处理大规模数据集时，选择小部分数据集进行抽样运算，从而减少资源浪费。</p>\n",
            "tags": [
                "大数据技术与应用",
                "Hive"
            ]
        },
        {
            "id": "http://example.com/2021/11/30/%E5%9C%A8%E7%BA%BF%E8%AF%BE%E7%A8%8B/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8/Hive%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/",
            "url": "http://example.com/2021/11/30/%E5%9C%A8%E7%BA%BF%E8%AF%BE%E7%A8%8B/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8/Hive%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/",
            "title": "Hive安装教程",
            "date_published": "2021-11-30T14:51:49.025Z",
            "content_html": "<h1 id=\"hive的安装\"><a class=\"anchor\" href=\"#hive的安装\">#</a> Hive 的安装</h1>\n<p>​\t本地和远程模式安装配置方式大致相同，本质上是将 Hive 默认的元数据存储介质由自带的 Derby 数据库替换为 MySQL 数据库，这样无论在任何目录下以任何方式启动 Hive，只要连接的是同一台 Hive 服务，那么所有节点访问的元数据信息是一致的，从而实现元数据的共享。下面就以本地模式为例，讲解安装过程。<br />\n​\t本地模式的 Hive 安装主要包括两个步骤：首先安装 MySQL 服务，再安装 Hive。具体步骤如下：</p>\n<h2 id=\"安装mysql服务\"><a class=\"anchor\" href=\"#安装mysql服务\">#</a> 安装 MySQL 服务</h2>\n<pre><code>MySQL安装方式有许多种，可以直接解压安装包进行相关配置，也可以选择在线安装，本节选用在线安装MySQL方式。在线安装MySQL的具体指令和说明如下：\n</code></pre>\n<h3 id=\"切换yum源\"><a class=\"anchor\" href=\"#切换yum源\">#</a> 切换 yum 源</h3>\n<p><figure class=\"highlight bash\"><figcaption><span>命令行提示符</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@master ~]<span class=\"comment\"># vi /etc/yum.repos.d/CentOS-Base.repo</span></span><br></pre></td></tr></table></figure><br />\n 修改镜像的内容，一定要确定修改的是正确并且完整，修改的文本内容如下，删除的时候可以在命令模式下输入【100dd】快速删除，操作演示如下：<br />\n<img data-src=\"https://media.giphy.com/media/CdThtgNMrtTPh23QKN/source.gif\" alt=\"\" /><br />\n​\t需要修改的 yum 镜像文件如下也可以在下发的文件中进行复制：<br />\n<figure class=\"highlight txt\"><figcaption><span>yum镜像文件</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[base]</span><br><span class=\"line\">name=CentOS-6.10 - Base - mirrors.aliyun.com</span><br><span class=\"line\">failovermethod=priority</span><br><span class=\"line\">baseurl=http://mirrors.aliyun.com/centos-vault/6.10/os/$basearch/</span><br><span class=\"line\">gpgcheck=1</span><br><span class=\"line\">gpgkey=http://mirrors.aliyun.com/centos-vault/RPM-GPG-KEY-CentOS-6</span><br><span class=\"line\">#released updates </span><br><span class=\"line\">[updates]</span><br><span class=\"line\">name=CentOS-6.10 - Updates - mirrors.aliyun.com</span><br><span class=\"line\">failovermethod=priority</span><br><span class=\"line\">baseurl=http://mirrors.aliyun.com/centos-vault/6.10/updates/$basearch/</span><br><span class=\"line\">gpgcheck=1</span><br><span class=\"line\">gpgkey=http://mirrors.aliyun.com/centos-vault/RPM-GPG-KEY-CentOS-6</span><br><span class=\"line\">#additional packages that may be useful</span><br><span class=\"line\">[extras]</span><br><span class=\"line\">name=CentOS-6.10 - Extras - mirrors.aliyun.com</span><br><span class=\"line\">failovermethod=priority</span><br><span class=\"line\">baseurl=http://mirrors.aliyun.com/centos-vault/6.10/extras/$basearch/</span><br><span class=\"line\">gpgcheck=1</span><br><span class=\"line\">gpgkey=http://mirrors.aliyun.com/centos-vault/RPM-GPG-KEY-CentOS-6</span><br><span class=\"line\">#additional packages that extend functionality of existing packages</span><br><span class=\"line\">[centosplus]</span><br><span class=\"line\">name=CentOS-6.10 - Plus - mirrors.aliyun.com</span><br><span class=\"line\">failovermethod=priority</span><br><span class=\"line\">baseurl=http://mirrors.aliyun.com/centos-vault/6.10/centosplus/$basearch/</span><br><span class=\"line\">gpgcheck=1</span><br><span class=\"line\">enabled=0</span><br><span class=\"line\">gpgkey=http://mirrors.aliyun.com/centos-vault/RPM-GPG-KEY-CentOS-6</span><br><span class=\"line\">#contrib - packages by Centos Users</span><br><span class=\"line\">[contrib]</span><br><span class=\"line\">name=CentOS-6.10 - Contrib - mirrors.aliyun.com</span><br><span class=\"line\">failovermethod=priority</span><br><span class=\"line\">baseurl=http://mirrors.aliyun.com/centos-vault/6.10/contrib/$basearch/</span><br><span class=\"line\">gpgcheck=1</span><br><span class=\"line\">enabled=0</span><br><span class=\"line\">gpgkey=http://mirrors.aliyun.com/centos-vault/RPM-GPG-KEY-CentOS-6</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"清除yum缓存\"><a class=\"anchor\" href=\"#清除yum缓存\">#</a> 清除 yum 缓存</h3>\n<p><figure class=\"highlight bash\"><figcaption><span>Bash命令</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@master ~]<span class=\"comment\"># yum clean all</span></span><br></pre></td></tr></table></figure><br />\n<img data-src=\"https://i.loli.net/2021/11/30/jdJSKHGmwD97g5b.jpg\" alt=\"hive1.jpg\" /></p>\n<h3 id=\"在线安装mysql软件\"><a class=\"anchor\" href=\"#在线安装mysql软件\">#</a> 在线安装 MySql 软件</h3>\n<p><figure class=\"highlight bash\"><figcaption><span>Bash命令</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@master ~]<span class=\"comment\"># yum -y install mysql-server</span></span><br></pre></td></tr></table></figure><br />\n​\t安装完成后，如图片所示，则数据库安装完成：<br />\n<img data-src=\"https://i.loli.net/2021/11/30/oPD2nesHX7tJ9jy.jpg\" alt=\"hive2.jpg\" /></p>\n<h3 id=\"启动mysql服务\"><a class=\"anchor\" href=\"#启动mysql服务\">#</a> 启动 mysql 服务</h3>\n<p><figure class=\"highlight bash\"><figcaption><span>Bash命令</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@master ~]<span class=\"comment\"># service mysqld start</span></span><br><span class=\"line\">[root@master ~]<span class=\"comment\"># chkconfig mysqld on</span></span><br></pre></td></tr></table></figure><br />\n<img data-src=\"https://i.loli.net/2021/11/30/ZCIJv4xWKR53Dap.jpg\" alt=\"hive3.jpg\" /><br />\n​\t上述指令中，首先通过 “yum install” 命令下载并安装 MySQL 程序，并且启动 MySQL 服务，然后就可以使用 MySQL 命令连接到 MySQL 客户端。<strong>需要注意的是，上述安装与启动 MySQL 程序仅限于 Centos6 中使用。</strong><br />\n<strong>说明</strong>：如果是 CentOS7 可以参考：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9zZWdtZW50ZmF1bHQuY29tL2EvMTE5MDAwMDAyMjI0NTg1OQ==\">Centos7 在线安装和配置 MySQL5.7 - SegmentFault 思否</span></p>\n<h3 id=\"修改登录mysql用户名和密码bash\"><a class=\"anchor\" href=\"#修改登录mysql用户名和密码bash\">#</a> 修改登录 MySQL 用户名和密码 bash</h3>\n<p>​\t进入 mysql 数据库<br />\n <figure class=\"highlight bash\"><figcaption><span>Bash命令</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@master ~]<span class=\"comment\"># mysql</span></span><br></pre></td></tr></table></figure><br />\n<img data-src=\"https://i.loli.net/2021/11/30/6f2azJbwBKVpAsn.jpg\" alt=\"hive4.jpg\" /></p>\n<p><figure class=\"highlight plain\"><figcaption><span>数据库界面</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mysql&gt; set password for root@localhost&#x3D;password(&#39;123456&#39;);</span><br><span class=\"line\">Query OK, 0 rows affected (0.00 sec)</span><br><span class=\"line\"></span><br><span class=\"line\">mysql&gt; GRANT ALL PRIVILEGES ON *.* TO &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39; WITH GRANT OPTION;</span><br><span class=\"line\">Query OK, 0 rows affected (0.00 sec)</span><br><span class=\"line\"></span><br><span class=\"line\">mysql&gt; FLUSH PRIVILEGES;</span><br><span class=\"line\">Query OK, 0 rows affected (0.00 sec)</span><br></pre></td></tr></table></figure><br />\n​\t结果显示如下：<br />\n<img data-src=\"https://i.loli.net/2021/11/30/aHubCKIYEwZ1RVc.jpg\" alt=\"hive5.jpg\" /><br />\n​\t运行结束后，【ctrl+c】退出 mysql 数据库！</p>\n<h2 id=\"安装hive\"><a class=\"anchor\" href=\"#安装hive\">#</a> 安装 Hive</h2>\n<h3 id=\"上传hive安装包并解压\"><a class=\"anchor\" href=\"#上传hive安装包并解压\">#</a> 上传 Hive 安装包并解压</h3>\n<p>​\t创建出存放 hive 软件的目录和安装的目录，切换至 <code>software</code>  目录，将软件包拖放至此，然后在进行解压到 <code>servers</code> 。<br />\n<figure class=\"highlight bash\"><figcaption><span>Bash命令</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@master ~]<span class=\"comment\"># mkdir -p /export/software/</span></span><br><span class=\"line\">[root@master ~]<span class=\"comment\"># mkdir -p /export/servers/</span></span><br><span class=\"line\">[root@master ~]<span class=\"comment\"># cd /export/software/</span></span><br><span class=\"line\">[root@master software]</span><br></pre></td></tr></table></figure><br />\n​\t打开 <code>新建文件传输</code> 图标，如下所示：<br />\n<img data-src=\"https://i.loli.net/2021/11/30/C2ZpIGaFubx1w5E.jpg\" alt=\"hive6.jpg\" /><br />\n<figure class=\"highlight bash\"><figcaption><span>Bash命令</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@master software]<span class=\"comment\"># tar -zxvf apache-hive-1.2.1-bin.tar.gz -C /export/servers/</span></span><br></pre></td></tr></table></figure><br />\n​\t<strong>一定要确定是解压成功之后在操作，解压成功会有如下信息显示。</strong><br />\n<img data-src=\"https://i.loli.net/2021/11/30/boZ3HIk2WvY8Bgq.jpg\" alt=\"hive7.jpg\" /></p>\n<h3 id=\"修改配置文件\"><a class=\"anchor\" href=\"#修改配置文件\">#</a> 修改配置文件</h3>\n<p>​\t首先，切换目录到 hive 配置文件目录下，由于 Hive 安装包 conf 目录下，没有提供 hive-site.xml 文件，这里需要创建并编辑一个 hive-site.xml 配置文件，具体内容如下所示：<br />\n<figure class=\"highlight bash\"><figcaption><span>Bash命令</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@master servers]<span class=\"comment\"># cd /export/servers/apache-hive-1.2.1-bin/conf/</span></span><br><span class=\"line\">[root@master conf]<span class=\"comment\"># vi hive-site.xml</span></span><br></pre></td></tr></table></figure><br />\n​\t将以下配置信息粘贴进去，并保存退出。<br />\n<figure class=\"highlight xml\"><figcaption><span>hive配置文件</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">configuration</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span>Mysql连接协议<span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>com.mysql.jdbc.Driver<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span>JDBC连接驱动<span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>root<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span>用户名<span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>123456<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span>密码<span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">configuration</span>&gt;</span>\t </span><br></pre></td></tr></table></figure></p>\n<h3 id=\"依赖包的更换\"><a class=\"anchor\" href=\"#依赖包的更换\">#</a> 依赖包的更换</h3>\n<p>​\t将桌面的 <code>mysql-connector-java-5.1.21.jar</code> Mysql 驱动包放到 <code>/export/servers/apache-hive-1.2.1-bin/lib</code>  目录中，如下图所示：<br />\n<img data-src=\"https://i.loli.net/2021/11/30/O9w4fybUYhj7vsF.jpg\" alt=\"hive8.jpg\" /><br />\n​\t开两个 <code>新建文件传输窗口</code> 将 Hadoop 中的 Jline 包替换成 hive 包中 Jline 包，并将<strong> Hadoop 中旧的 Jline 包版本删除，一定要删除，否则还是旧版本。</strong><br />\n<img data-src=\"https://i.loli.net/2021/11/30/UE6eOuVZBP9Lzts.jpg\" alt=\"hive9.jpg\" /><br />\n<strong> 注意：如果拖不过去的话，就先将 Jline 包拖放到桌面，然后再放进 Hadoop 文件夹中。</strong></p>\n<h3 id=\"启动hadoop集群\"><a class=\"anchor\" href=\"#启动hadoop集群\">#</a> 启动 Hadoop 集群</h3>\n<p><figure class=\"highlight bash\"><figcaption><span>Bash命令</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@master conf]<span class=\"comment\"># sh /usr/local/hadoop-2.6.5/sbin/start-all.sh</span></span><br><span class=\"line\">WARNING: HADOOP_SECURE_DN_USER has been replaced by HDFS_DATANODE_SECURE_USER. Using value of HADOOP_SECURE_DN_USER.</span><br><span class=\"line\">Starting namenodes on [master]</span><br><span class=\"line\">Last login: Thu Nov 25 05:04:04 CST 2021 from 192.168.128.1 on pts/0</span><br><span class=\"line\">Starting datanodes</span><br><span class=\"line\">Last login: Thu Nov 25 05:43:45 CST 2021 on pts/0</span><br><span class=\"line\">Starting secondary namenodes [master]</span><br><span class=\"line\">Last login: Thu Nov 25 05:43:48 CST 2021 on pts/0</span><br><span class=\"line\">Starting resourcemanager</span><br><span class=\"line\">Last login: Thu Nov 25 05:43:53 CST 2021 on pts/0</span><br><span class=\"line\">Starting nodemanagers</span><br><span class=\"line\">Last login: Thu Nov 25 05:44:00 CST 2021 on pts/0</span><br><span class=\"line\">[root@master ~]<span class=\"comment\"># jps</span></span><br><span class=\"line\">1824 SecondaryNameNode</span><br><span class=\"line\">2375 Jps</span><br><span class=\"line\">2059 ResourceManager</span><br><span class=\"line\">1566 NameNode</span><br></pre></td></tr></table></figure><br />\n 初始化 Mysql 元数据<br />\n <figure class=\"highlight bash\"><figcaption><span>Bash命令</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@master conf]<span class=\"comment\"># cd /export/servers/apache-hive-1.2.1-bin/bin/</span></span><br><span class=\"line\">[root@master bin]<span class=\"comment\"># ./schematool -dbType mysql -initSchema</span></span><br><span class=\"line\">Metastore connection URL:\t jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=<span class=\"literal\">true</span></span><br><span class=\"line\">Metastore Connection Driver :\t com.mysql.jdbc.Driver</span><br><span class=\"line\">Metastore connection User:\t root</span><br><span class=\"line\">Starting metastore schema initialization to 1.2.0</span><br><span class=\"line\">Initialization script hive-schema-1.2.0.mysql.sql</span><br><span class=\"line\">Initialization script completed</span><br><span class=\"line\">schemaTool completed</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"启动hive\"><a class=\"anchor\" href=\"#启动hive\">#</a> 启动 Hive</h3>\n<p><figure class=\"highlight bash\"><figcaption><span>Bash命令</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@master bin]<span class=\"comment\"># ./hive</span></span><br><span class=\"line\"></span><br><span class=\"line\">Logging initialized using configuration <span class=\"keyword\">in</span> jar:file:/<span class=\"built_in\">export</span>/servers/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties</span><br><span class=\"line\">hive&gt; show databases;</span><br><span class=\"line\">OK</span><br><span class=\"line\">default</span><br><span class=\"line\">Time taken: 0.901 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure><br />\n​\t到此为止，Hive 已经安装完成！！</p>\n",
            "tags": [
                "大数据技术与应用",
                "Hive"
            ]
        }
    ]
}